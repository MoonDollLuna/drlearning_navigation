# PPO (Proximal Policy Optimization)
# PPO agent training configuration
# Facebook / Adapted by Luna Jimenez Fernandez

# The PPO agent and configuration file was developed by the Facebook AI Research Group
# This file has just been adapted to use the developed config file (to ensure parity with
# the reactive navigation agent training)

# Hyperparameters and ResNet18 from on https://arxiv.org/abs/2012.0611

VERBOSE: False

BASE_TASK_CONFIG_PATH: "configs/base_config.yaml"
TRAINER_NAME: "ppo"
ENV_NAME: "NavRLEnv"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: []

SENSORS: ["DEPTH_SENSOR"]

CHECKPOINT_FOLDER: "Training/PPO/Checkpoints"
EVAL_CKPT_PATH_DIR: "Training/PPO/Checkpoints"
TRAINING_LOG_FOLDER: "Training/PPO/Log"

# TOTAL_NUM_STEPS: 75e6
NUM_UPDATES: 15000
LOG_INTERVAL: 25
NUM_CHECKPOINTS: 100
# Evaluate on all episodes
TEST_EPISODE_COUNT: -1

# Force PyTorch to be single threaded as
# this improves performance considerably
FORCE_TORCH_SINGLE_THREADED: True
NUM_ENVIRONMENTS: 6


RL:
  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 4
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.5
    num_steps: 128
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: True
    use_linear_lr_decay: True
    reward_window_size: 50

    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False
