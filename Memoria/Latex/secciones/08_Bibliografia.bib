% CAPITULO 1

@article{Jimenez2020,
address = {Albacete},
author = {Jimenez, Luna},
file = {:C\:/Users/Luna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jimenez Fernandez - 2020 - Aplicaci{\'{o}}n de Deep Reinforcement Learning a un juego real - Tetris.pdf:pdf},
institution = {Universidad de Castilla-La Mancha},
journal = {Universidad de Castilla-La Mancha},
mendeley-groups = {Others / Self},
pages = {109},
title = {{Aplicaci{\'{o}}n de Deep Reinforcement Learning a un juego real - Tetris}},
url = {https://github.com/MoonDollLuna/dqlearning-tetris},
year = {2020}
}


@article{Sampedro2018,
abstract = {Navigation in unknown indoor environments with fast collision avoidance capabilities is an ongoing research topic. Traditional motion planning algorithms rely on precise maps of the environment, where re-adapting a generated path can be highly demanding in terms of computational cost. In this paper, we present a fast reactive navigation algorithm using Deep Reinforcement Learning applied to multi rotor aerial robots. Taking as input the 2D-laser range measurements and the relative position of the aerial robot with respect to the desired goal, the proposed algorithm is successfully trained in a Gazebo-based simulation scenario by adopting an artificial potential field formulation. A thorough evaluation of the trained agent has been carried out both in simulated and real indoor scenarios, showing the appropriate reactive navigation behavior of the agent in the presence of static and dynamic obstacles.},
author = {Sampedro, Carlos and Bavle, Hriday and Rodriguez-Ramos, Alejandro and {De La Puente}, Paloma and Campoy, Pascual},
doi = {10.1109/IROS.2018.8593706},
file = {:C\:/Users/Luna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sampedro et al. - 2018 - Laser-Based Reactive Navigation for Multirotor Aerial Robots using Deep Reinforcement Learning.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Aerial Systems: Perception and Autonomy,Model Learning for Control,Reactive and Sensor-Based Planning},
mendeley-groups = {Others / Self},
pages = {1024--1031},
title = {{Laser-Based Reactive Navigation for Multirotor Aerial Robots using Deep Reinforcement Learning}},
year = {2018}
}

@article{szot2021habitat,
  title     =     {Habitat 2.0: Training Home Assistants to Rearrange their Habitat},
  author    =     {Andrew Szot and Alex Clegg and Eric Undersander and Erik Wijmans and Yili Zhao and John Turner and Noah Maestre and Mustafa Mukadam and Devendra Chaplot and Oleksandr Maksymets and Aaron Gokaslan and Vladimir Vondrus and Sameer Dharur and Franziska Meier and Wojciech Galuba and Angel Chang and Zsolt Kira and Vladlen Koltun and Jitendra Malik and Manolis Savva and Dhruv Batra},
  journal   =     {arXiv preprint arXiv:2106.14405},
  year      =     {2021}
}

@inproceedings{habitat19iccv,
  title     =     {Habitat: {A} {P}latform for {E}mbodied {AI} {R}esearch},
  author    =     {Manolis Savva and Abhishek Kadian and Oleksandr Maksymets and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},
  booktitle =     {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      =     {2019}
}

% CAPITULO 2
@article{DBLP:journals/corr/abs-1807-06757,
  author    = {Peter Anderson and
               Angel X. Chang and
               Devendra Singh Chaplot and
               Alexey Dosovitskiy and
               Saurabh Gupta and
               Vladlen Koltun and
               Jana Kosecka and
               Jitendra Malik and
               Roozbeh Mottaghi and
               Manolis Savva and
               Amir Roshan Zamir},
  title     = {On Evaluation of Embodied Navigation Agents},
  journal   = {CoRR},
  volume    = {abs/1807.06757},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.06757},
  archivePrefix = {arXiv},
  eprint    = {1807.06757},
  timestamp = {Mon, 04 Mar 2019 08:31:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-06757.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{habitat2020sim2real,
  title     =     {Sim2{R}eal {P}redictivity: {D}oes {E}valuation in {S}imulation {P}redict {R}eal-{W}orld {P}erformance?},
  author    =     {{Abhishek Kadian*} and {Joanne Truong*} and Aaron Gokaslan and Alexander Clegg and Erik Wijmans and Stefan Lee and Manolis Savva and Sonia Chernova and Dhruv Batra},
  journal   =   {IEEE Robotics and Automation Letters},
  year      =   {2020},
  volume    =   {5},
  number    =   {4},
  pages     =   {6670-6677},
}

@inproceedings{batra2020objectnav,
  title     =     {Object{N}av {R}evisited: {O}n {E}valuation of {E}mbodied {A}gents {N}avigating to {O}bjects},
  author    =     {Dhruv Batra and Aaron Gokaslan and Aniruddha Kembhavi and Oleksandr Maksymets and Roozbeh Mottaghi and Manolis Savva and Alexander Toshev and Erik Wijmans},
  booktitle =     {arXiv:2006.13171},
  year      =     {2020}
}

@article{DBLP:journals/corr/abs-2004-05155,
  author    = {Devendra Singh Chaplot and
               Dhiraj Gandhi and
               Saurabh Gupta and
               Abhinav Gupta and
               Ruslan Salakhutdinov},
  title     = {Learning to Explore using Active Neural {SLAM}},
  journal   = {CoRR},
  volume    = {abs/2004.05155},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.05155},
  archivePrefix = {arXiv},
  eprint    = {2004.05155},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05155.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2008-09285,
  author    = {Santhosh K. Ramakrishnan and
               Ziad Al{-}Halah and
               Kristen Grauman},
  title     = {Occupancy Anticipation for Efficient Exploration and Navigation},
  journal   = {CoRR},
  volume    = {abs/2008.09285},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.09285},
  archivePrefix = {arXiv},
  eprint    = {2008.09285},
  timestamp = {Fri, 28 Aug 2020 12:11:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-09285.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2009-03231,
  author    = {Samyak Datta and
               Oleksandr Maksymets and
               Judy Hoffman and
               Stefan Lee and
               Dhruv Batra and
               Devi Parikh},
  title     = {Integrating Egocentric Localization for More Realistic Point-Goal
               Navigation Agents},
  journal   = {CoRR},
  volume    = {abs/2009.03231},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.03231},
  archivePrefix = {arXiv},
  eprint    = {2009.03231},
  timestamp = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-03231.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Partsey2021,
author = {Partsey, Ruslan},
file = {:C\:/Users/Luna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Partsey - Unknown - Robust Visual Odometry for Realistic PointGoal Navigation.pdf:pdf},
journal = {Ukranian Catholic University},
pages = {87},
title = {{Robust Visual Odometry for Realistic PointGoal Navigation}},
year = {2021},
url={https://er.ucu.edu.ua/handle/1/2703}
}

% CAPITULO 3

@book{alma991004256519704990,
  added-at = {2020-02-01T18:23:11.000+0100},
  author = {Russell, Stuart and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/20533b732950d1c5ab4ac12d4f32fe637/mialhoma},
  edition = 3,
  interhash = {53908a52dd4c6c8e39f93f4ffc8341be},
  intrahash = {0533b732950d1c5ab4ac12d4f32fe637},
  keywords = {ties4530},
  publisher = {Prentice Hall},
  timestamp = {2020-02-01T18:23:11.000+0100},
  title = {Artificial Intelligence: A Modern Approach},
  year = 2010
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{10.5555/3161223,
author = {Buduma, Nikhil and Locascio, Nicholas},
title = {Fundamentals of Deep Learning: Designing Next-Generation Machine Intelligence Algorithms},
year = {2017},
isbn = {1491925612},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {With the reinvigoration of neural networks in the 2000s, deep learning has become
an extremely active area of research, one thats paving the way for modern machine
learning. In this practical book, author Nikhil Buduma provides examples and clear
explanations to guide you through major concepts of this complicated field. Companies
such as Google, Microsoft, and Facebook are actively growing in-house deep-learning
teams. For the rest of us, however, deep learning is still a pretty complex and difficult
subject to grasp. If youre familiar with Python, and have a background in calculus,
along with a basic understanding of machine learning, this book will get you started.
Examine the foundations of machine learning and neural networks Learn how to train
feed-forward neural networks Use Tensor Flow to implement your first neural network
Manage problems that arise as you begin to make networks deeper Build neural networks
that analyze complex images Perform effective dimensionality reduction using autoencoders
Dive deep into sequence analysis to examine language Understand the fundamentals of
reinforcement learning}
}

@article{Zhang2003ArtificialNN,
  title={Artificial neural networks for RF and microwave design - from theory to practice},
  author={Qi-jun Zhang and K. Gupta and V. Devabhaktuni},
  journal={IEEE Transactions on Microwave Theory and Techniques},
  year={2003},
  volume={51},
  pages={1339-1350}
}

@article{Rumelhart:1986we,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  journal = {Nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Learning Representations by Back-propagating Errors}},
  url = {http://www.nature.com/articles/323533a0},
  volume = 323,
  year = 1986
}

@INBOOK{gradientKolen,
  author={Kolen, John F. and Kremer, Stefan C.},
  booktitle={A Field Guide to Dynamical Recurrent Networks}, 
  title={Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies}, 
  year={2001},
  volume={},
  number={},
  pages={237-243},
  doi={10.1109/9780470544037.ch14}
}

@misc{cswiki18, 
title={Max-pooling / Pooling}, 
url={https://computersciencewiki.org/index.php/Max-pooling_/_Pooling}, 
journal={Computer Science Wiki}, 
publisher={Computer Science Wiki}, 
author={Computer Science Wiki}, 
year={2018}, 
month={Feb}
} 

@article{Shorten2019ASO,
  title={A survey on Image Data Augmentation for Deep Learning},
  author={Connor Shorten and T. Khoshgoftaar},
  journal={Journal of Big Data},
  year={2019},
  volume={6},
  pages={1-48}
}

@article{10.1162/neco.1989.1.4.541,
    author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
    title = {Backpropagation Applied to Handwritten Zip Code Recognition},
    year = {1989},
    issue_date = {Winter 1989},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {1},
    number = {4},
    issn = {0899-7667},
    url = {https://doi.org/10.1162/neco.1989.1.4.541},
    doi = {10.1162/neco.1989.1.4.541},
    abstract = {The ability of learning networks to generalize can be greatly enhanced by providing
    constraints from the task domain. This paper demonstrates how such constraints can
    be integrated into a backpropagation network through the architecture of the network.
    This approach has been successfully applied to the recognition of handwritten zip
    code digits provided by the U.S. Postal Service. A single network learns the entire
    recognition operation, going from the normalized image of the character to the final
    classification.},
    journal = {Neural Comput.},
    month = dec,
    pages = {541–551},
    numpages = {11}
    }

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second Edition},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@book{sutton1998rli,
  abstract = {This introductory textbook on reinforcement learning is targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems, and we hope it will also be of interest to psychologists and neuroscientists. It also contains several new results. An html version is available.},
  added-at = {2008-09-26T11:05:09.000+0200},
  edition = {First Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/222063d43301bdff5387f812f3d59499b/yish},
  interhash = {9369ad4b6ab9e5aa15fb456143e9a09d},
  intrahash = {22063d43301bdff5387f812f3d59499b},
  keywords = {computational learning machine reinforcement supervised wleformativeeassessment},
  publisher = {MIT Press},
  timestamp = {2008-09-26T11:05:09.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://www.cs.ualberta.ca/~sutton/book/the-book.html},
  year = 1998
}

@article{Zhou1988ComputationOO,
  title={Computation of optical flow using a neural network},
  author={Y. Zhou and R. Chellappa},
  journal={IEEE 1988 International Conference on Neural Networks},
  year={1988},
  pages={71-78 vol.2}
}

@phdthesis{qlearning,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Watkins, C. J. C. H.},
  biburl = {https://www.bibsonomy.org/bibtex/21ffd549077ea1da7675431a17fa2af03/idsia},
  citeulike-article-id = {2381652},
  interhash = {ca824d64b71939208358edb4a26f8351},
  intrahash = {1ffd549077ea1da7675431a17fa2af03},
  keywords = {juergen},
  priority = {2},
  school = {King's College, Oxford},
  timestamp = {2008-03-11T14:53:51.000+0100},
  title = {Learning from Delayed Rewards},
  year = 1989
}

@techreport{sarsa,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, England},
  author = {Rummery, G. A. and Niranjan, M.},
  biburl = {https://www.bibsonomy.org/bibtex/220239f82583859588c96171ccc015e65/idsia},
  citeulike-article-id = {2378892},
  institution = {Cambridge University Engineering Department},
  interhash = {0c7cd3821ad0fe1b39a6ce1b35ec4bc0},
  intrahash = {20239f82583859588c96171ccc015e65},
  keywords = {nn},
  number = {TR 166},
  priority = {2},
  timestamp = {2008-03-11T14:59:39.000+0100},
  title = {On-Line {Q}-Learning Using Connectionist Systems},
  year = 1994
}

@article{Witten1977AnAO,
  title={An Adaptive Optimal Controller for Discrete-Time Markov Environments},
  author={I. Witten},
  journal={Inf. Control.},
  year={1977},
  volume={34},
  pages={286-295}
}



@article{DBLP:journals/corr/abs-1811-12560,
  author    = {Vincent Fran{\c{c}}ois{-}Lavet and
               Peter Henderson and
               Riashat Islam and
               Marc G. Bellemare and
               Joelle Pineau},
  title     = {An Introduction to Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1811.12560},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12560},
  archivePrefix = {arXiv},
  eprint    = {1811.12560},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-12560.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS1995_fd06b8ea,
 author = {Gordon, Geoffrey J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M. C. Mozer and M. Hasselmo},
 pages = {},
 publisher = {MIT Press},
 title = {Stable Fitted Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/1995/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf},
 volume = {8},
 year = {1996}
}

@InProceedings{10.1007/11564096_32,
author="Riedmiller, Martin",
editor="Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
title="Neural Fitted Q Iteration -- First Experiences with a Data Efficient Neural Reinforcement Learning Method",
booktitle="Machine Learning: ECML 2005",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="317--328",
abstract="This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.",
isbn="978-3-540-31692-3"
}




% DQL
@article{Mnih2015HumanlevelCT,
  title={Human-level control through deep reinforcement learning},
  author={V. Mnih and K. Kavukcuoglu and D. Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and A. Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and A. Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},
  journal={Nature},
  year={2015},
  volume={518},
  pages={529-533}
}

@article{DBLP:journals/corr/abs-1710-02298,
  author    = {Matteo Hessel and
               Joseph Modayil and
               Hado van Hasselt and
               Tom Schaul and
               Georg Ostrovski and
               Will Dabney and
               Daniel Horgan and
               Bilal Piot and
               Mohammad Gheshlaghi Azar and
               David Silver},
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1710.02298},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.02298},
  archivePrefix = {arXiv},
  eprint    = {1710.02298},
  timestamp = {Mon, 13 Aug 2018 16:48:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-02298.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{doubleqlearning,
author = {Van Hasselt, Hado},
year = {2010},
month = {01},
pages = {2613-2621},
title = {Double Q-learning.}
}

@article{DBLP:journals/corr/HasseltGS15,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  archivePrefix = {arXiv},
  eprint    = {1509.06461},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{schaul2015prioritized,
  abstract = {Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. DQN with prioritized
experience replay achieves a new state-of-the-art, outperforming DQN with
uniform replay on 41 out of 49 games.},
  added-at = {2019-11-17T22:17:21.000+0100},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  biburl = {https://www.bibsonomy.org/bibtex/2be47ebfc4c5eb8f06b374fe632a3e236/jan.hofmann1},
  description = {[1511.05952] Prioritized Experience Replay},
  interhash = {db6e4402b0807938aae61afc45b70f73},
  intrahash = {be47ebfc4c5eb8f06b374fe632a3e236},
  keywords = {final reinforcement_learning thema:double_dqn},
  note = {cite arxiv:1511.05952Comment: Published at ICLR 2016},
  timestamp = {2019-12-09T10:13:58.000+0100},
  title = {Prioritized Experience Replay},
  url = {http://arxiv.org/abs/1511.05952},
  year = 2015
}

@article{DBLP:journals/corr/WangFL15,
  author    = {Ziyu Wang and
               Nando de Freitas and
               Marc Lanctot},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06581},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06581},
  archivePrefix = {arXiv},
  eprint    = {1511.06581},
  timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WangFL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1023/A:1022633531479,
author = {Sutton, Richard S.},
title = {Learning to Predict by the Methods of Temporal Differences},
year = {1988},
issue_date = {August 1988},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {3},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1022633531479},
doi = {10.1023/A:1022633531479},
abstract = {This article introduces a class of incremental learning procedures specialized for
prediction – that is, for using past experience with an incompletely known system
to predict its future behavior. Whereas conventional prediction-learning methods assign
credit by means of the difference between predicted and actual outcomes, the new methods
assign credit by means of the difference between temporally successive predictions.
Although such temporal-difference methods have been used in Samuel's checker player,
Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained
poorly understood. Here we prove their convergence and optimality for special cases
and relate them to supervised-learning methods. For most real-world prediction problems,
temporal-difference methods require less memory and less peak computation than conventional
methods and they produce more accurate predictions. We argue that most problems to
which supervised learning is currently applied are really prediction problems of the
sort to which temporal-difference methods can be applied to advantage.},
journal = {Mach. Learn.},
month = aug,
pages = {9–44},
numpages = {36},
keywords = {Incremental learning, evaluation functions, prediction, credit assignment, connectionism}
}

@article{DBLP:journals/corr/BellemareDM17,
  author    = {Marc G. Bellemare and
               Will Dabney and
               R{\'{e}}mi Munos},
  title     = {A Distributional Perspective on Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1707.06887},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06887},
  archivePrefix = {arXiv},
  eprint    = {1707.06887},
  timestamp = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BellemareDM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/FortunatoAPMOGM17,
  author    = {Meire Fortunato and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Jacob Menick and
               Ian Osband and
               Alex Graves and
               Vlad Mnih and
               R{\'{e}}mi Munos and
               Demis Hassabis and
               Olivier Pietquin and
               Charles Blundell and
               Shane Legg},
  title     = {Noisy Networks for Exploration},
  journal   = {CoRR},
  volume    = {abs/1706.10295},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.10295},
  archivePrefix = {arXiv},
  eprint    = {1706.10295},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FortunatoAPMOGM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% AGENT CRITIC
@article{DBLP:journals/corr/MnihBMGLHSK16,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  archivePrefix = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{PETERS2008682,
title = {Reinforcement learning of motor skills with policy gradients},
journal = {Neural Networks},
volume = {21},
number = {4},
pages = {682-697},
year = {2008},
note = {Robotics and Neuroscience},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2008.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608008000701},
author = {Jan Peters and Stefan Schaal},
keywords = {Reinforcement learning, Policy gradient methods, Natural gradients, Natural Actor-Critic, Motor skills, Motor primitives},
abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g., by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm.}
}


@InProceedings{pmlr-v37-schulman15,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vitay_2020, 
title={Deep Reinforcement Learning},
url={https://julien-vitay.net/deeprl/DeepRL.html}, 
journal={Deep Reinforcement Learning}, 
publisher={Julien Vitay}, 
author={Vitay, Julien}, 
year={2020}
} 

@inproceedings{journals/corr/LillicrapHPHETS15,
  added-at = {2019-07-12T20:04:55.000+0200},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  biburl = {https://www.bibsonomy.org/bibtex/22708c349821330660afb992aec2be5d1/lanteunis},
  booktitle = {ICLR},
  editor = {Bengio, Yoshua and LeCun, Yann},
  ee = {http://arxiv.org/abs/1509.02971},
  interhash = {b791167abe535c8525f6a9bf62fcc1ab},
  intrahash = {2708c349821330660afb992aec2be5d1},
  keywords = {},
  timestamp = {2019-07-12T20:04:55.000+0200},
  title = {Continuous control with deep reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#LillicrapHPHETS15},
  year = 2016
}

@article{DBLP:journals/corr/abs-1804-08617,
  author    = {Gabriel Barth{-}Maron and
               Matthew W. Hoffman and
               David Budden and
               Will Dabney and
               Dan Horgan and
               Dhruva TB and
               Alistair Muldal and
               Nicolas Heess and
               Timothy P. Lillicrap},
  title     = {Distributed Distributional Deterministic Policy Gradients},
  journal   = {CoRR},
  volume    = {abs/1804.08617},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.08617},
  eprinttype = {arXiv},
  eprint    = {1804.08617},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-08617.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% ROBOTICA

@book{10.5555/3152585,
author = {Corke, Peter},
title = {Robotics, Vision and Control: Fundamental Algorithms In MATLAB, Second Edition},
year = {2017},
isbn = {3319544128},
publisher = {Springer Publishing Company, Incorporated},
edition = {2nd},
abstract = {Robotic vision, the combination of robotics and computer vision, involves the application
of computer algorithms to data acquired from sensors. The research community has developed
a large body of such algorithms but for a newcomer to the field this can be quite
daunting. For over 20 years the author has maintained two open-source MATLAB Toolboxes,
one for robotics and one for vision. They provide implementations of many important
algorithms and allow users to work with real problems, not just trivial examples.
This book makes the fundamental algorithms of robotics, vision and control accessible
to all. It weaves together theory, algorithms and examples in a narrative that covers
robotics and computer vision separately and together. Using the latest versions of
the Toolboxes the author shows how complex problems can be decomposed and solved using
just a few simple lines of code. The topics covered are guided by real problems observed
by the author over many years as a practitioner of both robotics and computer vision.
It is written in an accessible but informative style, easy to read and absorb, and
includes over 1000 MATLAB and Simulink examples and over 400 figures. The book is
a real walk through the fundamentals of mobile robots, arm robots. then camera models,
image processing, feature extraction and multi-view geometry and finally bringing
it all together with an extensive discussion of visual servo systems. This second
edition is completely revised, updated and extended with coverage of Lie groups, matrix
exponentials and twists; inertial navigation; differential drive robots; lattice planners;
pose-graph SLAM and map making; restructured material on arm-robot kinematics and
dynamics; series-elastic actuators and operational-space control; Lab color spaces;
light field cameras; structured light, bundle adjustment and visual odometry; and
photometric visual servoing. An authoritative book, reaching across fields, thoughtfully
conceived and brilliantly accomplished! OUSSAMA KHATIB, Stanford}
}

@book{mappingexploration,
author = {Stachniss, Cyrill},
year = {2009},
month = {01},
pages = {},
title = {Robotic Mapping and Exploration},
volume = {55},
isbn = {978-3-642-01096-5},
doi = {10.1007/978-3-642-01097-2}
}

@article{journals/algorithmica/LumelskyS87,
  added-at = {2011-05-18T00:00:00.000+0200},
  author = {Lumelsky, Vladimir J. and Stepanov, Alexander A.},
  biburl = {https://www.bibsonomy.org/bibtex/2fe0a27ecaac6f5c9e05150e84d10570b/dblp},
  ee = {http://dx.doi.org/10.1007/BF01840369},
  interhash = {c2f3a6736cc1ef34f16d813843ec5f2a},
  intrahash = {fe0a27ecaac6f5c9e05150e84d10570b},
  journal = {Algorithmica},
  keywords = {dblp},
  pages = {403-430},
  timestamp = {2011-05-19T11:35:14.000+0200},
  title = {Path-Planning Strategies for a Point Mobile Automaton Moving Amidst Unknown Obstacles of Arbitrary Shape.},
  url = {http://dblp.uni-trier.de/db/journals/algorithmica/algorithmica2.html#LumelskyS87},
  volume = 2,
  year = 1987
}

@book{lavalle:2006,
  added-at = {2006-10-17T00:27:45.000+0200},
  author = {Lavalle, Steven M.},
  biburl = {https://www.bibsonomy.org/bibtex/2d52a7489c7a4cfe32e48f949a171c0c0/mobst},
  interhash = {cb59968f0fee7daa3b701405fbce22e2},
  intrahash = {d52a7489c7a4cfe32e48f949a171c0c0},
  isbn = {0521862051},
  keywords = {imported},
  price = {EUR 53,90},
  publisher = {Cambridge University Press},
  timestamp = {2006-10-17T00:27:45.000+0200},
  title = {Planning Algorithms},
  year = 2006
}


@INPROCEEDINGS{131810,  
author={Koren, Y. and Borenstein, J.},  
booktitle={Proceedings. 1991 IEEE International Conference on Robotics and Automation},   
title={Potential field methods and their inherent limitations for mobile robot navigation},   
year={1991},  
volume={},  
number={},  
pages={1398-1404 vol.2},  
doi={10.1109/ROBOT.1991.131810}
}

@book{andrews1983impedance,
  title={Impedance Control as a Framework for Implementing Obstacle Avoidance in a Manipulator},
  author={Andrews, J.R.},
  url={https://books.google.es/books?id=OcmcNwAACAAJ},
  year={1983},
  publisher={Massachusetts Institute of Technology, Department of Mechanical Engineering}
}

@article{doi:10.1177/027836498900800406,
author = {Ronald C. Arkin},
title ={Motor Schema — Based Mobile Robot Navigation},
journal = {The International Journal of Robotics Research},
volume = {8},
number = {4},
pages = {92-112},
year = {1989},
doi = {10.1177/027836498900800406},

URL = { 
        https://doi.org/10.1177/027836498900800406
    
},
}

@INPROCEEDINGS{291936,  
author={Quinlan, S. and Khatib, O.},  
booktitle={[1993] Proceedings IEEE International Conference on Robotics and Automation},   
title={Elastic bands: connecting path planning and control},   
year={1993},  
volume={},  
number={},  
pages={802-807 vol.2},  
doi={10.1109/ROBOT.1993.291936}}

@article{Tai2017VirtualtorealDR,
  title={Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation},
  author={L. Tai and Giuseppe Paolo and Ming Liu},
  journal={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2017},
  pages={31-36}
}

% CAPITULO 4

@article{Matterport3D,
author = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
journal = {International Conference on 3D Vision (3DV)},
mendeley-groups = {Conclusiones,Datasets},
title = {{Matterport3D: Learning from RGB-D Data in Indoor Environments}},
year = {2017}
}

@inproceedings{xiazamirhe2018gibsonenv,
author = {Xia, Fei and {R. Zamir}, Amir and He, Zhi-Yang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
booktitle = {Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on},
mendeley-groups = {Datasets},
organization = {IEEE},
title = {{Gibson {Env}: real-world perception for embodied agents}},
year = {2018}
}

@article{DBLP:journals/corr/abs-2104-01111,
  author    = {Xiaojun Chang and
               Pengzhen Ren and
               Pengfei Xu and
               Zhihui Li and
               Xiaojiang Chen and
               Alex Hauptmann},
  title     = {Scene Graphs: {A} Survey of Generations and Applications},
  journal   = {CoRR},
  volume    = {abs/2104.01111},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.01111},
  archivePrefix = {arXiv},
  eprint    = {2104.01111},
  timestamp = {Mon, 19 Apr 2021 12:42:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-01111.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1712-03931,
  author    = {Manolis Savva and
               Angel X. Chang and
               Alexey Dosovitskiy and
               Thomas A. Funkhouser and
               Vladlen Koltun},
  title     = {{MINOS:} Multimodal Indoor Simulator for Navigation in Complex Environments},
  journal   = {CoRR},
  volume    = {abs/1712.03931},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.03931},
  archivePrefix = {arXiv},
  eprint    = {1712.03931},
  timestamp = {Mon, 13 Aug 2018 16:48:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-03931.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1906-05797,
  author    = {Julian Straub and
               Thomas Whelan and
               Lingni Ma and
               Yufan Chen and
               Erik Wijmans and
               Simon Green and
               Jakob J. Engel and
               Raul Mur{-}Artal and
               Carl Ren and
               Shobhit Verma and
               Anton Clarkson and
               Mingfei Yan and
               Brian Budge and
               Yajie Yan and
               Xiaqing Pan and
               June Yon and
               Yuyang Zou and
               Kimberly Leon and
               Nigel Carter and
               Jesus Briales and
               Tyler Gillingham and
               Elias Mueggler and
               Luis Pesqueira and
               Manolis Savva and
               Dhruv Batra and
               Hauke M. Strasdat and
               Renzo De Nardi and
               Michael Goesele and
               Steven Lovegrove and
               Richard A. Newcombe},
  title     = {The Replica Dataset: {A} Digital Replica of Indoor Spaces},
  journal   = {CoRR},
  volume    = {abs/1906.05797},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05797},
  archivePrefix = {arXiv},
  eprint    = {1906.05797},
  timestamp = {Tue, 25 Jun 2019 11:12:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-05797.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{eqa_matterport,
  title={{E}mbodied {Q}uestion {A}nswering in {P}hotorealistic {E}nvironments with {P}oint {C}loud {P}erception},
  author={Erik Wijmans and Samyak Datta and Oleksandr Maksymets and Abhishek Das and Georgia Gkioxari and Stefan Lee and Irfan Essa and Devi Parikh and Dhruv Batra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@misc{habitatmp3d, 
url={https://aihabitat.org/datasets/hm3d/0002.html}, 
title={{H}abitat {M}atterport {D}ataset},
journal={Habitat Matterport Dataset}, 
publisher={Facebook AI Research}, 
author={Batra, Dhruv and Chang, Angel and Clegg, Alex and Gokaslan, Aaron and Goluba, Wojtek and Maksymets, Oleksandr and Savva, Manolis and Kumar, Santhosh and Turner, John and Undersander, Eric and et al.}, year={2021}
} 

@misc{yaml, 
url={https://yaml.org/}, 
title={{T}he {O}fficial {YAML} {W}eb {S}ite},
author={Evans, Clark and Ben-Kiki, Oren and Döt Net, Iggy}, 
year={2001}
} 

% CAPITULO 5

@article{Krizhevsky2012ImageNetCW,
author = {Krizhevsky, A and Sutskever, Ilya and Hinton, Geoffrey E},
journal = {Communications of the ACM},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
volume = {60},
year = {2012}
}

@inproceedings{Glorot2010UnderstandingTD,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Xavier Glorot and Yoshua Bengio},
  booktitle={AISTATS},
  year={2010}
}

@article{adam2014,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@article{Salimans2016,
abstract = {We present weight normalization: a reparameterization of the weight vectors
in a neural network that decouples the length of those weight vectors from
their direction. By reparameterizing the weights in this way we improve the
conditioning of the optimization problem and we speed up convergence of
stochastic gradient descent. Our reparameterization is inspired by batch
normalization but does not introduce any dependencies between the examples in a
minibatch. This means that our method can also be applied successfully to
recurrent models such as LSTMs and to noise-sensitive applications such as deep
reinforcement learning or generative models, for which batch normalization is
less well suited. Although our method is much simpler, it still provides much
of the speed-up of full batch normalization. In addition, the computational
overhead of our method is lower, permitting more optimization steps to be taken
in the same amount of time. We demonstrate the usefulness of our method on
applications in supervised image recognition, generative modelling, and deep
reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1602.07868},
author = {Salimans, Tim and Kingma, Diederik P.},
eprint = {1602.07868},
file = {:C\:/Users/Luna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salimans, Kingma - 2016 - Weight Normalization A Simple Reparameterization to Accelerate Training of Deep Neural Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {901--909},
publisher = {Neural information processing systems foundation},
title = {{Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}},
url = {https://arxiv.org/abs/1602.07868v3},
year = {2016}
}

@Article{sym12091526,
AUTHOR = {Ahsan, Md Manjurul and E. Alam, Tasfiq and Trafalis, Theodore and Huebner, Pedro},
TITLE = {Deep MLP-CNN Model Using Mixed-Data to Distinguish between COVID-19 and Non-COVID-19 Patients},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1526},
URL = {https://www.mdpi.com/2073-8994/12/9/1526},
ISSN = {2073-8994},
ABSTRACT = {The limitations and high false-negative rates (30%) of COVID-19 test kits have been a prominent challenge during the 2020 coronavirus pandemic. Manufacturing those kits and performing the tests require extensive resources and time. Recent studies show that radiological images like chest X-rays can offer a more efficient solution and faster initial screening of COVID-19 patients. In this study, we develop a COVID-19 diagnosis model using Multilayer Perceptron and Convolutional Neural Network (MLP-CNN) for mixed-data (numerical/categorical and image data). The model predicts and differentiates between COVID-19 and non-COVID-19 patients, such that early diagnosis of the virus can be initiated, leading to timely isolation and treatments to stop further spread of the disease. We also explore the benefits of using numerical/categorical data in association with chest X-ray images for screening COVID-19 patients considering both balanced and imbalanced datasets. Three different optimization algorithms are used and tested:adaptive learning rate optimization algorithm (Adam), stochastic gradient descent (Sgd), and root mean square propagation (Rmsprop). Preliminary computational results show that, on a balanced dataset, a model trained with Adam can distinguish between COVID-19 and non-COVID-19 patients with a higher accuracy of 96.3%. On the imbalanced dataset, the model trained with Rmsprop outperformed all other models by achieving an accuracy of 95.38%. Additionally, our proposed model outperformed selected existing deep learning models (considering only chest X-ray or CT scan images) by producing an overall average accuracy of 94.6% ± 3.42%.},
DOI = {10.3390/sym12091526}
}

@article{DBLP:journals/corr/abs-2010-00717,
  author    = {Yanyu Zhang},
  title     = {Deep Reinforcement Learning with Mixed Convolutional Network},
  journal   = {CoRR},
  volume    = {abs/2010.00717},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.00717},
  archivePrefix = {arXiv},
  eprint    = {2010.00717},
  timestamp = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-00717.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HeZR015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  volume    = {abs/1502.01852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.01852},
  archivePrefix = {arXiv},
  eprint    = {1502.01852},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZR015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1214/aoms/1177703732,
author = {Peter J. Huber},
title = {{Robust Estimation of a Location Parameter}},
volume = {35},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {73 -- 101},
year = {1964},
doi = {10.1214/aoms/1177703732},
URL = {https://doi.org/10.1214/aoms/1177703732}
}
