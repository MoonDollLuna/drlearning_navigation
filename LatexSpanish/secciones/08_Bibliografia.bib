@techreport{Buitelaar2003,
author = {Buitelaar, Paul and Cimiano, Philipp and Magnini, Bernardo},
file = {:C$\backslash$:/Users/Luna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buitelaar, Cimiano, Magnini - 2003 - Ontology Learning from Text An Overview(2).pdf:pdf},
keywords = {Knowledge Acquisition,Ontology Learning,Text Mining},
publisher = {Book Title Book Editors IOS Press},
title = {{Ontology Learning from Text: An Overview}},
url = {http://www-sop.inria.fr/acacia/WORKSHOPS/ECAI2002-OLT/},
year = {2003}
}

@techreport{JimenezFernandez2020,
abstract = {El algoritmo de Deep Q-Learning es el estado del arte actualmente para pro- blemas de aprendizaje por refuerzo, siendo especialmente notable cuando se aplica a juegos. Por tanto, el objetivo de este trabajo es el an{\'{a}}lisis e implementaci{\'{o}}n de esta t{\'{e}}cnica para estudiar su viabilidad aplicada a Tetris. La memoria comienza con el estudio de la t{\'{e}}cnica de Deep Q-Learning, que surge a Deep Learning (el uso de redes neuronales profundas para poder resolver problemas complejos) y Q-Learning partir de la uni{\'{o}}n de dos t{\'{e}}cnicas de aprendizaje autom{\'{a}}tico: (uno de los principales algoritmos de aprendizaje por refuerzo sin modelo). Esta t{\'{e}}cnica tiene adem{\'{a}}s varias posibles mejoras por aplicar, de las cuales se describir{\'{a}} Prioritized Experience Replay , una de las mejoras m{\'{a}}s importantes y notables actualmente. El juego sobre el que se implementar{\'{a}} esta t{\'{e}}cnica es Tetris. Se comentar{\'{a}} el fun- cionamiento de este juego, junto a las decisiones que se han tomado en su dise{\~{n}}o (incluyendo el sistema de puntuaci{\'{o}}n, el sistema de dicultad, la generaci{\'{o}}n de piezas o las simplicaciones tomadas para facilitar la aplicaci{\'{o}}n de la t{\'{e}}cnica) y su imple- mentaci{\'{o}}n (realizando una implementaci{\'{o}}n propia para facilitar la labor al algoritmo y permitir la adaptaci{\'{o}}n total a las necesidades de {\'{e}}ste). Para poder aplicar el algoritmo es necesario formalizar varios aspectos del conoci- miento. Se describir{\'{a}}n las dos aproximaciones que se han hecho a este conocimiento: una primera aproximaci{\'{o}}n basada en imitar las entradas que realizar{\'{i}}a un jugador hu- mano en el juego, y una segunda aproximaci{\'{o}}n fundamentada en el razonamiento de un jugador humano durante el juego (centr{\'{a}}ndose en d{\'{o}}nde colocar las piezas en juego para maximizar la puntuaci{\'{o}}n). De cada una de estas aproximaciones se comentar{\'{a}}n los principales elementos a caracterizar para poder resolver el problema (el estado, la acci{\'{o}}n y las recompensas otorgadas a los agentes). Tambi{\'{e}}n se describir{\'{a}}n los agentes utilizados por cada apro- ximaci{\'{o}}n (haciendo hincapi{\'{e}} en sus arquitecturas internas, su funcionamiento...) y c{\'{o}}mo se han implementado estos agentes en el juego. Finalmente, se estudiar{\'{a}} el rendimiento de los agentes durante el aprendizaje y en partidas reales, estudiando la evoluci{\'{o}}n de las l{\'{i}}neas completadas, la puntuaci{\'{o}}n obtenida y la longevidad de los agentes (m{\'{e}}tricas {\'{u}}tiles para evaluar la idoneidad de estos agentes como jugadores de Tetris) y comparando estos rendimientos entre ellos, buscando analizar los resultados y extraer conclusiones.},
author = {{Jimenez Fernandez}, Luna},
file = {:home/luna/Escritorio/TFM/LunaJimenezFernandez-DQLTetris-MemoriaTFG.pdf:pdf},
institution = {Universidad de Castilla-La Mancha},
mendeley-groups = {Others / Self},
pages = {109},
title = {{Aplicaci{\'{o}}n de Deep Reinforcement Learning a un juego real - Tetris}},
year = {2020}
}

